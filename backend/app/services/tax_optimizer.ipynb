{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class TaxOptimizationEnv(gym.Env):\n",
    "    def __init__(self, data, deduction_policy):\n",
    "        super(TaxOptimizationEnv, self).__init__()\n",
    "        self.data = data.sample(frac=1).reset_index(drop=True)  # Shuffle data\n",
    "        self.deduction_policy = deduction_policy\n",
    "        self.current_index = 0  # Track transaction index\n",
    "        \n",
    "        # Initialize deduction limits\n",
    "        self.deduction_limits = {rule[\"category\"]: rule[\"max_limit\"] for rule in deduction_policy[\"deductions\"]}\n",
    "        \n",
    "        # Action space: 0 = Don't deduct, 1 = Deduct\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observation space: 5 features + deduction limits\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5 + len(self.deduction_limits),), dtype=np.float32)\n",
    "        \n",
    "        self.state = self._get_next_transaction()\n",
    "        self.episode_ended = False\n",
    "\n",
    "    def _get_next_transaction(self):\n",
    "        \"\"\"Fetch the next transaction and convert to a state vector.\"\"\"\n",
    "        if self.current_index >= len(self.data):\n",
    "            self.current_index = 0  # Restart from beginning\n",
    "            self.data = self.data.sample(frac=1).reset_index(drop=True)  # Shuffle again\n",
    "        \n",
    "        row = self.data.iloc[self.current_index]\n",
    "        self.current_index += 1\n",
    "        \n",
    "        # Normalize deduction limits\n",
    "        deduction_limits = np.array([self.deduction_limits[cat] for cat in self.deduction_limits], dtype=np.float32)\n",
    "        deduction_limits /= max(deduction_limits)  # Normalize to [0, 1]\n",
    "        \n",
    "        return np.concatenate([\n",
    "            np.array([\n",
    "                row[\"amount\"],  \n",
    "                row[\"category\"],  \n",
    "                row[\"merchant\"],  \n",
    "                row[\"payment_method\"],  \n",
    "                row[\"tax_deductible\"]  \n",
    "            ], dtype=np.float32),\n",
    "            deduction_limits\n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self._get_next_transaction()\n",
    "        self.episode_ended = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        # Get transaction details\n",
    "        amount = self.state[0]\n",
    "        category = self.state[1]\n",
    "        tax_deductible = self.state[4]\n",
    "        \n",
    "        # Get deduction rule for the category\n",
    "        deduction_rule = next(rule for rule in self.deduction_policy[\"deductions\"] if rule[\"category\"] == category)\n",
    "        rate = deduction_rule[\"rate\"]\n",
    "        max_limit = deduction_rule[\"max_limit\"]\n",
    "        \n",
    "        # Calculate reward\n",
    "        if action == 1:  # Deduct\n",
    "            if tax_deductible == 1 and self.deduction_limits[category] >= amount * rate:\n",
    "                reward = amount * rate  # Reward = tax savings\n",
    "                self.deduction_limits[category] -= amount * rate\n",
    "            else:\n",
    "                reward = -10  # Penalty for invalid deduction\n",
    "        else:  # Don't deduct\n",
    "            reward = 0\n",
    "        \n",
    "        self.state = self._get_next_transaction()\n",
    "        done = False  # In this case, episodes can continue indefinitely\n",
    "        return self.state, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 304      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0044   |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 285      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00518  |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n",
      "Total Tax Savings: [271.40906]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../../data/freelancer_tax_deductions.csv\")\n",
    "\n",
    "# Initialize or update LabelEncoders\n",
    "label_encoders = {}\n",
    "\n",
    "for col in [\"category\", \"merchant\", \"payment_method\"]:\n",
    "    if col not in label_encoders:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "    \n",
    "    # Fit on the entire column (including unseen categories)\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "# Normalize amount\n",
    "df[\"amount\"] = (df[\"amount\"] - df[\"amount\"].min()) / (df[\"amount\"].max() - df[\"amount\"].min())\n",
    "\n",
    "# Load deduction policy\n",
    "with open(\"../../data/deduction_policy.json\", \"r\") as f:\n",
    "    deduction_policy = json.load(f)\n",
    "\n",
    "# Environment Class\n",
    "class TaxOptimizationEnv(gym.Env):\n",
    "    def __init__(self, df, deduction_policy, label_encoders):\n",
    "        super(TaxOptimizationEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.deduction_policy = deduction_policy\n",
    "        self.label_encoders = label_encoders\n",
    "        self.current_index = 0\n",
    "\n",
    "        # State space: (category, merchant, amount, payment_method, tax_deductible)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "\n",
    "        # Action space: 0 = Accept Transaction, 1 = Claim Deduction\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_index >= len(self.df):\n",
    "            return np.zeros(5)\n",
    "\n",
    "        row = self.df.iloc[self.current_index]\n",
    "        return np.array([\n",
    "            row[\"category\"] / max(1, len(self.label_encoders[\"category\"].classes_)),  # Normalize category\n",
    "            row[\"merchant\"] / max(1, len(self.label_encoders[\"merchant\"].classes_)),  # Normalize merchant\n",
    "            row[\"amount\"],  # Already normalized\n",
    "            row[\"payment_method\"] / max(1, len(self.label_encoders[\"payment_method\"].classes_)),  # Normalize payment method\n",
    "            1.0 if row[\"category\"] in [self.label_encoders[\"category\"].transform([d[\"category\"]])[0] for d in self.deduction_policy[\"deductions\"] if d[\"category\"] in self.label_encoders[\"category\"].classes_] else 0.0\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.df.iloc[self.current_index]\n",
    "        category = row[\"category\"]\n",
    "        amount = row[\"amount\"]\n",
    "\n",
    "        # Try to find a matching category in deduction policy\n",
    "        deduction_rule = next(\n",
    "            (rule for rule in self.deduction_policy[\"deductions\"] \n",
    "             if rule[\"category\"] in self.label_encoders[\"category\"].classes_ and \n",
    "             self.label_encoders[\"category\"].transform([rule[\"category\"]])[0] == category),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        reward = 0\n",
    "        if deduction_rule:\n",
    "            rate = deduction_rule[\"rate\"]\n",
    "            max_limit = deduction_rule[\"max_limit\"]\n",
    "            reward = min(amount * rate, max_limit) if action == 1 else 0\n",
    "\n",
    "        self.current_index += 1\n",
    "        done = self.current_index >= len(self.df)\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "# Create environment\n",
    "env = TaxOptimizationEnv(df, deduction_policy, label_encoders)\n",
    "env = DummyVecEnv([lambda: env])  # Stable-Baselines3 requires vectorized environments\n",
    "\n",
    "# Initialize the DQN agent\n",
    "model = DQN('MlpPolicy', env, learning_rate=1e-3, buffer_size=10000, batch_size=64, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate the model\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(1000):  # Simulate 1000 steps\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "print(f\"Total Tax Savings: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_savings = total_reward * 1000  # Adjust based on your dataset's scale\n",
    "\n",
    "# Generate the tax report with actual savings\n",
    "report_id = generate_tax_report(tax_savings)\n",
    "\n",
    "print(f\"Total Tax Savings: ${tax_savings:,.2f}\")\n",
    "print(f\"Tax Report Generated: https://docs.google.com/document/d/{report_id}/edit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for indiviual users the code above is just for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from tax_report_generator import generate_tax_report\n",
    "\n",
    "# Load deduction policy\n",
    "with open(\"../../data/deduction_policy.json\", \"r\") as f:\n",
    "    deduction_policy = json.load(f)\n",
    "\n",
    "class TaxOptimizationEnv(gym.Env):\n",
    "    def __init__(self, df, deduction_policy, label_encoders):\n",
    "        super(TaxOptimizationEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.deduction_policy = deduction_policy\n",
    "        self.label_encoders = label_encoders\n",
    "        self.current_index = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        if self.current_index >= len(self.df):\n",
    "            return np.zeros(5)\n",
    "        \n",
    "        row = self.df.iloc[self.current_index]\n",
    "        return np.array([\n",
    "            row[\"category\"] / len(self.label_encoders[\"category\"].classes_),\n",
    "            row[\"merchant\"] / len(self.label_encoders[\"merchant\"].classes_),\n",
    "            row[\"amount\"],  \n",
    "            row[\"payment_method\"] / len(self.label_encoders[\"payment_method\"].classes_),\n",
    "            1.0 if row[\"category\"] in [self.label_encoders[\"category\"].transform([d[\"category\"]])[0] for d in self.deduction_policy[\"deductions\"]] else 0.0\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.df.iloc[self.current_index]\n",
    "        category = row[\"category\"]\n",
    "        amount = row[\"amount\"]\n",
    "        \n",
    "        deduction_rule = next(\n",
    "            (rule for rule in self.deduction_policy[\"deductions\"] if self.label_encoders[\"category\"].transform([rule[\"category\"]])[0] == category),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        reward = 0\n",
    "        if deduction_rule:\n",
    "            rate = deduction_rule[\"rate\"]\n",
    "            max_limit = deduction_rule[\"max_limit\"]\n",
    "            reward = min(amount * rate, max_limit) if action == 1 else 0\n",
    "        \n",
    "        self.current_index += 1\n",
    "        done = self.current_index >= len(self.df)\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "def calculate_user_tax_savings(user_id, user_name, user_transactions):\n",
    "    \"\"\"Calculate tax savings for an individual user and generate a report.\"\"\"\n",
    "    df = pd.DataFrame(user_transactions)  # Convert user transactions to DataFrame\n",
    "    \n",
    "    # Normalize data if necessary (ensure consistency with training)\n",
    "    df[\"amount\"] = (df[\"amount\"] - df[\"amount\"].min()) / (df[\"amount\"].max() - df[\"amount\"].min())\n",
    "\n",
    "    label_encoders = {}\n",
    "    for col in [\"category\", \"merchant\", \"payment_method\"]:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "    env = TaxOptimizationEnv(df, deduction_policy, label_encoders)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    model = DQN.load(\"tax_savings_model\")  # Load pre-trained model\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(len(user_transactions)):  # Process all transactions\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "\n",
    "    tax_savings = total_reward * 1000  # Adjust based on real-world scale\n",
    "    report_id = generate_tax_report(user_name, tax_savings)\n",
    "\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"user_name\": user_name,\n",
    "        \"tax_savings\": tax_savings,\n",
    "        \"report_link\": f\"https://docs.google.com/document/d/{report_id}/edit\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"example_user_123\"  # Replace with actual user ID\n",
    "user_name = \"John Doe\"  # Replace with actual user name\n",
    "user_transactions = get_user_transactions(user_id)\n",
    "\n",
    "if user_transactions:\n",
    "    tax_savings_result = calculate_user_tax_savings(user_id, user_name, user_transactions)\n",
    "    print(tax_savings_result)\n",
    "else:\n",
    "    print(\"No transactions found for this user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed receipt data\n",
    "df = pd.read_csv(\"receipts_data.csv\")\n",
    "\n",
    "# Continue with tax optimization\n",
    "env = TaxOptimizationEnv(df, deduction_policy, label_encoders)\n",
    "env = DummyVecEnv([lambda: env])  # Stable-Baselines3 requires vectorized environments\n",
    "\n",
    "# Train and evaluate as usual\n",
    "model = DQN('MlpPolicy', env, learning_rate=1e-3, buffer_size=10000, batch_size=64, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "print(f\"Total Tax Savings: {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
